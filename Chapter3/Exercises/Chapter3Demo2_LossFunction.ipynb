{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMF9+/orur6dGYM/ysY9va5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"QlK66H8P28P9"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","from tensorflow.keras import Input, Model\n","import numpy as np\n","\n","# Parameters\n","\n","embedding_dim = 64\n","latent_dim = 32\n","batch_size = 32\n","epochs = 10\n","\n","# Dummy Text Data\n","vocab_size = 1000\n","max_length = 10\n","dummy_data = np.random.randint(0, vocab_size, size=(100, max_length))\n"]},{"cell_type":"code","source":["\n","# Encoder Architecture\n","input_text = Input(shape=(max_length,))\n","x = Embedding(vocab_size, embedding_dim)(input_text)   # Embedding layer\n","x = None   # LSTM layer with latent_dim\n","z_mean = None  # Dense layer with 2 units\n","z_log_var = None  # Dense layer with 2 units\n"],"metadata":{"id":"b5Vll0yr7d9j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Custom loss function\n","def vae_loss(z_mean, z_log_var):\n","    kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)  # KL loss\n","    return tf.reduce_mean(kl_loss)\n","\n","# Add KL loss to the model\n","kl_loss = None  # Custom loss function applied on z_mean and z_log_var\n","encoder = None  # Create encoder model with outputs z_mean and z_log_var\n","encoder.add_loss(kl_loss)"],"metadata":{"id":"-ilvTuK37f7h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compile the Model\n","optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n","encoder.compile(optimizer=optimizer)\n","\n","# Learning Rate Scheduler\n","def scheduler(epoch, lr):\n","    # Reduce the lr every epoch by e^(-0.1)\n","    return None\n","\n","callback = None  # Learning Rate Scheduler\n","\n","# Train the Model\n","encoder.fit(dummy_data, epochs=epochs, batch_size=batch_size, callbacks=[callback])\n"],"metadata":{"id":"MZRVaz7-7jUj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Simple evaluation: Check the difference in latent representations\n","def evaluate_latent_space(latent1, latent2):\n","    mean_diff = None  # Difference in means\n","    log_var_diff = None  # Difference in log variances\n","    return mean_diff, log_var_diff\n"],"metadata":{"id":"LGYwiwsE4tqW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sample Text Data (replace with your dataset)\n","texts = [\n","    \"The cat sat on the mat.\",\n","    \"The dog sat on the log.\",\n","    \"The sky is blue and the grass is green.\",\n","    \"Roses are red, violets are blue.\"\n","]\n","\n","# Assuming a previously defined vocab_size and max_length\n","vocab_size = 1000  # Example value\n","max_length = 10    # Example value\n","\n","# Tokenization and Padding\n","tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(texts)\n","sequences = tokenizer.texts_to_sequences(texts)\n","padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')"],"metadata":{"id":"ZymCD98Z7vHy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["latent_means, latent_log_vars = encoder.predict(padded_sequences)"],"metadata":{"id":"Ut8-NMwF86kp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["latent_means.shape"],"metadata":{"id":"xvSzFBcD780e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["latent_log_vars.shape"],"metadata":{"id":"nl2GK3m079t6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Assuming the first two are similar, and the last two are dissimilar\n","latent_similar_means = [latent_means[0], latent_means[1]]\n","latent_dissimilar_means = [latent_means[2], latent_means[3]]\n","latent_similar_log_vars = [latent_log_vars[0], latent_log_vars[1]]\n","latent_dissimilar_log_vars = [latent_log_vars[2], latent_log_vars[3]]\n"],"metadata":{"id":"nZJmw194_D4s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Evaluate\n","diff_similar = evaluate_latent_space(latent_similar_means, latent_similar_log_vars)\n","diff_dissimilar = evaluate_latent_space(latent_dissimilar_means, latent_dissimilar_log_vars)\n","\n","print(\"Difference in Latent Space for Similar Texts:\", diff_similar)\n","print(\"Difference in Latent Space for Dissimilar Texts:\", diff_dissimilar)"],"metadata":{"id":"KfVx9C_C_GTL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SNC570Vw_ILc"},"execution_count":null,"outputs":[]}]}