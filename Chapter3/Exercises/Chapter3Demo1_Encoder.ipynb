{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOGPj7ASbZnWNhE3DGK+nim"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lovWQsgI1vEC"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Lambda, Bidirectional\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras import backend as K\n"]},{"cell_type":"code","source":["max_seq_length = 100  # Length of input sequences\n","vocab_size = 10000    # Size of vocabulary\n","embedding_dim = 64    # Embedding dimensions\n","latent_dim = 32       # Latent space dimensions\n"],"metadata":{"id":"piG8faav2Sz3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Encoder\n","input_text = Input(shape=(max_seq_length,))  # Define the input layer with a specified maximum sequence length.\n","\n","x = None  # Embedding layer to convert input text into dense vector representations.\n","\n","encoder_outputs = None  # Define a bidirectional LSTM layer of latent_dim units. Only return the last state\n","\n","# VAE Sampling layer\n","def sampling(inputs):\n","    z_mean, z_log_sigma = inputs                  # Unpack the arguments: mean and log of variance of the latent space.\n","    batch = None                # Determine the batch size.\n","    dim = None                     # Determine the dimensionality of the latent space.\n","    epsilon = None  # Generate random values for epsilon, used for the reparameterization trick.\n","    return z_mean + tf.exp(0.5 * z_log_sigma) * epsilon  # Return the sampled latent vector (reparameterization trick).\n","\n","z_mean = None  # Dense layer to generate the mean of the latent space distribution.\n","z_log_sigma = None  # Dense layer to generate the log variance of the latent space distribution.\n","z = Lambda(sampling)([z_mean, z_log_sigma])  # Apply the sampling function to obtain the latent variable 'z'.\n","\n","# Instantiate encoder model\n","encoder = Model(input_text, [z_mean, z_log_sigma, z])  # Create the encoder model that outputs the mean, log variance, and the sampled z.\n","encoder.summary()  # Display the summary of the encoder model.\n"],"metadata":{"id":"y89dKHj32nWW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example text\n","texts = [\"Hello world\", \"Variational Autoencoders are fun\"]\n","\n","# Tokenize text\n","tokenizer = Tokenizer(num_words=vocab_size)\n","tokenizer.fit_on_texts(texts)\n","sequences = tokenizer.texts_to_sequences(texts)\n","\n","# Pad sequences\n","data = pad_sequences(sequences, maxlen=max_seq_length)\n"],"metadata":{"id":"QUtciuvT2xGb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wNUs0VdT2zqZ"},"execution_count":null,"outputs":[]}]}