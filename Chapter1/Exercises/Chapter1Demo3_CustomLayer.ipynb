{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMKc8Q7/17oK4B0kQl1ROVl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Custom Layers"],"metadata":{"id":"qxIJkTKx-3qA"}},{"cell_type":"markdown","source":["Creating a Keras model with multiple outputs and a custom sampling layer, similar to those used in Variational AutoEncoders (VAEs), is an interesting task. Additionally, we'll incorporate custom loss functions for each output. Here's how we can achieve this:\n","\n","Here's how we can do it:\n","\n","1. Define the Custom Sampling Layer: This layer will be used to sample from a probability distribution, typically used in VAEs.\n","\n","1. Define the Model Architecture: The model will include the custom sampling layer and have two outputs.\n","\n","1. Implement Custom Loss Functions:\n","        - Custom Binary Classification Loss: A simple example could be a variant of binary cross-entropy.\n","        - KL Divergence Loss: TensorFlow provides a function for KL Divergence, which we could use directly but in this case we will provide a variation\n","\n","1. Generate Dummy Data: We'll create dummy data appropriate for our model's input and output specifications.\n","\n","1. Compile and Train the Model: We'll compile the model with our custom loss functions and then train it.\n","\n"],"metadata":{"id":"6iKEdMsL-69n"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.losses import KLDivergence\n","import numpy as np\n","\n","# 1. Custom Sampling Layer\n","\n","class SamplingLayer(tf.keras.layers.Layer):\n","    def call(self, inputs):\n","        z_mean, z_log_var = None  # Extract z_mean, z_log_var\n","        batch = None  # Calculate batch_size from the shape of z_mean\n","        dim = None   # Calculate dimension  from the shape of z_mean\n","        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n","        return None   # Return the sampled data z_mean + e^(0.5*z_log_var)*epsilon\n","\n"],"metadata":{"id":"4ZJA14-H-6Yu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. Model architecture\n","input_layer = Input(shape=(10,))\n","dense_layer = Dense(64, activation='relu')(input_layer)\n","\n","# For the VAE-like output\n","z_mean = None  # Dense layer with 10 neurons\n","z_log_var = None   # Dense layer with 10 neurons\n","sampling_output = None  # Custom sampling layer\n","output1 = None  # Binary classification output from dense_layer\n","output2 = None  # Multiclass classification output using the sampled data\n","\n","model = None  # Create the model"],"metadata":{"id":"0uaLqKFxCtDH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. Custom binary classification loss\n","def custom_binary_loss(y_true, y_pred):\n","    y_true = tf.cast(y_true, tf.float32)\n","    epsilon = 1e-15\n","    y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon)\n","    return -tf.reduce_mean(y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))\n","\n","# 3. Custom KL Divergence Loss\n","def custom_kl_divergence_loss(y_true, y_pred, scale_factor=1.0):\n","    kl_loss = tf.keras.losses.KLDivergence()(y_true, y_pred)\n","    return scale_factor * kl_loss\n"],"metadata":{"id":"ird5Kzba_x_V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 4. Generate dummy data\n","x_dummy = np.random.random((1000, 10))\n","y_dummy_output1 = np.random.randint(2, size=(1000, 1))\n","y_dummy_output2 = np.random.randint(5, size=(1000, 5))\n"],"metadata":{"id":"7c_hK1DK_01U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 5. Compile the model\n","model.compile(optimizer='adam',\n","              loss={'output1': custom_binary_loss, 'output2': lambda y_true, y_pred: custom_kl_divergence_loss(y_true, y_pred, scale_factor=2.0)},\n","              metrics={'output1': ['accuracy'], 'output2': ['accuracy']})\n","\n","# 5. Train the model\n","model.fit(x_dummy, {'output1': y_dummy_output1, 'output2': y_dummy_output2}, epochs=10)"],"metadata":{"id":"KJ_Kw0cN_3ir"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once a Keras model is trained, you can evaluate its performance on a test dataset and use it to make predictions. Continuing from the previous example, I'll show you how to:\n","\n","1. Evaluate the Model: We'll evaluate the model on a separate set of dummy data to see how it performs.\n","\n","2. Use the Model for Prediction: We'll use the model to make predictions based on new input data."],"metadata":{"id":"i_x1B6c3_8W9"}},{"cell_type":"code","source":["# 1. Evaluate the model\n","\n","# Generate some dummy test data\n","x_dummy_test = np.random.random((200, 10))\n","y_dummy_test_output1 = np.random.randint(2, size=(200, 1))  # Binary labels\n","y_dummy_test_output2 = np.random.randint(5, size=(200, 5))  # One-hot encoded labels for 5 classes\n","\n","# Evaluate the model\n","evaluation = model.evaluate(x_dummy_test, {'output1': y_dummy_test_output1, 'output2': y_dummy_test_output2})\n","print(f\"Test Loss, Test Accuracy for Output 1: {evaluation[1]}, {evaluation[3]}\")\n","print(f\"Test Loss, Test Accuracy for Output 2: {evaluation[2]}, {evaluation[4]}\")\n"],"metadata":{"id":"dNf8dv9ZAB56"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. Use the model for prediction\n","\n","# New sample data for prediction\n","new_sample = np.random.random((1, 10))\n","\n","# Making predictions\n","predictions = model.predict(new_sample)\n","print(f\"Predictions for Output 1 (Binary classification): {predictions[0]}\")\n","print(f\"Predictions for Output 2 (Multiclass classification): {predictions[1]}\")\n"],"metadata":{"id":"22brxmbSAFmf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PGxalfR4BAUX"},"execution_count":null,"outputs":[]}]}