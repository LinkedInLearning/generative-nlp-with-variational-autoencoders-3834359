{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100",
   "authorship_tag": "ABX9TyN3bDB2qP1yKrnCTevEnF3S"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Create text like Shakespeare\n",
    "\n",
    "Welcome to an exciting journey where you'll learn how to generate text in the style of Shakespeare using a Variational Autoencoder (VAE)! This challenge is not just about coding, but about merging the worlds of literature and advanced machine learning. As you progress, you'll gain insights into natural language processing, neural networks, and the creative applications of AI.\n",
    "\n",
    "## Steps and Hints\n",
    "\n",
    "Here's an overview of the challenge with steps and hints to guide you:\n",
    "\n",
    "### Prep work\n",
    "\n",
    "1. **Splitting the Corpus into Segments**:\n",
    "    Your first task is to divide the Shakespearean text into segments of a maximum sequence length, ensuring no part exceeds this limit. This is crucial for managing memory and computational efficiency.\n",
    "\n",
    "    **Hint**: Consider using a sliding window approach to break the text into manageable pieces.\n",
    "\n",
    "1. **Building a Vocabulary**:\n",
    "    Create a vocabulary from the corpus. This involves identifying unique words and possibly filtering out less frequent ones to reduce complexity.\n",
    "    \n",
    "    **Hint**: Pay attention to the frequency of words; rare words might not contribute much to the overall style.\n",
    "\n",
    "1. **Filtering Low-Frequency Words**:\n",
    "    This step involves removing words from the corpus that don't meet a minimum frequency threshold. This helps in focusing on the most significant words.\n",
    "    \n",
    "    **Hint**: Choose a sensible threshold that balances vocabulary size and expressiveness.\n",
    "\n",
    "1. **Data Loading and Preprocessing**:\n",
    "    Load and preprocess the data for the model. This step is key to ensuring your data is in the right format for training.\n",
    "    \n",
    "    **Hint**: Preprocessing might include normalizing and tokenizing\n",
    "    text.\n",
    "\n",
    "1. **Tokenization**:\n",
    "    Convert the corpus into a series of tokens (integers) based on word frequency. This numerical representation is what the model will actually process.\n",
    "    \n",
    "    **Hint**: Use established tokenization libraries to save time.\n",
    "\n",
    "1. **Updating Tokenizer for Vocabulary**:\n",
    "    Adjust your tokenizer to only include words present in your vocabulary. This step ensures consistency between the data and the model.\n",
    "    \n",
    "    **Hint**: This is about aligning your tokenizer settings with the vocabulary you've created.\n",
    "\n",
    "1. **Padding Sequences**:\n",
    "    Ensure all sequences are of uniform length by padding them. This uniformity is crucial for training neural networks.\n",
    "    \n",
    "    **Hint**: You can pad sequences with zeros to a predefined maximum length.\n",
    "\n",
    "1. **Main Execution**:\n",
    "    This is where you bring everything together: load the corpus, preprocess the data, and get the processed corpus, sequences, and tokenizer ready for training.\n",
    "    \n",
    "    **Hint**: Ensure all previous steps are correctly implemented before proceeding.\n",
    "\n",
    "1. **Preparing TensorFlow Dataset**:\n",
    "    Create a TensorFlow dataset from your sequences. This format is optimal for training models in TensorFlow.\n",
    "    \n",
    "    **Hint**: Take advantage of TensorFlow's data pipeline optimizations for efficiency.\n",
    "\n",
    "1. **Shuffling and Batching the Dataset**:\n",
    "    Shuffle the dataset for randomness and batch it according to a specified size. This step is crucial for effective training.\n",
    "    \n",
    "    **Hint**: A good shuffle ensures the model doesn't learn the order of the data, while batching affects memory usage and training speed.\n",
    "\n",
    "### Creating the VAE Model\n",
    "\n",
    "1. **Designing the Encoder**:\n",
    "    The encoder part of your VAE will process the input text and encode it into a latent (hidden) space. This involves designing a neural network architecture suitable for handling text data.\n",
    "    \n",
    "    **Hint**: Consider using layers like LSTM or GRU as they are effective in capturing the sequential nature of text.\n",
    "\n",
    "1. **Designing the Decoder**:\n",
    "    The decoder will take the encoded representation and reconstruct the original text. This part mirrors the encoder but in reverse.\n",
    "    \n",
    "    **Hint**: The decoder's architecture should complement the encoder, and it often uses similar layers.\n",
    "\n",
    "1. **Defining the Latent Space**:\n",
    "    The latent space is where your model learns to represent the data in a compressed form. Here, you'll define how the model represents this space, including the size and how it's sampled.\n",
    "    \n",
    "    **Hint**: The dimensionality of the latent space is a key hyperparameter. Experiment with different sizes to see what works best for your data.\n",
    "\n",
    "1. **Loss Function and Regularization**:\n",
    "    A critical part of training VAEs is defining the loss function, which often includes a reconstruction loss and a regularization term (like KL divergence) to maintain a well-formed latent space.\n",
    "    \n",
    "    **Hint**: Balancing these two aspects of the loss function is crucial for good model performance.\n",
    "\n",
    "###Training the Model\n",
    "\n",
    "1. **Setting up Training Parameters**:\n",
    "    Define parameters like learning rate, batch size, and number of epochs. These hyperparameters significantly impact how well and how quickly your model learns.\n",
    "    \n",
    "    **Hint**: Start with commonly used values and then fine-tune based on your model's performance.\n",
    "\n",
    "1. **Model Training**:\n",
    "    Train the model using your prepared dataset. Monitor the loss and adjust parameters as needed for optimal learning.\n",
    "    **Hint**: Use callbacks like ModelCheckpoint and EarlyStopping to manage long training times and avoid overfitting.\n",
    "\n",
    "###Evaluating the Model\n",
    "\n",
    "1. **Performance Metrics**:\n",
    "    Evaluate the model using appropriate metrics. For a VAE, you might look at reconstruction error and how well the model generalizes.\n",
    "    \n",
    "    **Hint**: Besides quantitative metrics, qualitative evaluation (like visually inspecting the generated text) is also valuable.\n",
    "\n",
    "1. **Fine-Tuning**:\n",
    "    Based on the evaluation, you may need to fine-tune the model. This could involve adjusting the architecture, training longer, or changing hyperparameters.\n",
    "    \n",
    "    **Hint**: Keep an experimental log to track changes and their effects.\n",
    "\n",
    "###Generating Text with Sampling\n",
    "\n",
    "1. **Sampling from the Latent Space**:\n",
    "    Generate new text by sampling points in the latent space and passing them through the decoder.\n",
    "    \n",
    "    \n",
    "    **Hint**: Explore different regions of the latent space to see the variety of texts your model can generate.\n",
    "\n",
    "1. **Decoding Generated Samples**:\n",
    "    The decoder will output a sequence of tokens that you'll need to convert back into text.\n",
    "\n",
    "    **Hint**: Ensure your tokenization process is reversible for accurate text generation.\n",
    "\n",
    "1. **Iterative Refinement (Optional)**:\n",
    "    You might refine the generated text by iteratively feeding it back into the model for further processing.\n",
    "    \n",
    "    **Hint**: This can sometimes improve coherence and style consistency.\n",
    "\n",
    "Remember, working with VAEs, especially for text generation, can be as much an art as a science. Don't hesitate to experiment with different approaches, and enjoy the process of creating something unique. As you proceed, you'll not only develop a deeper understanding of VAEs and NLP but also get a taste of how AI can be used creatively to mimic human-like artistic expressions. Happy modeling!"
   ],
   "metadata": {
    "id": "tgogSwmeAvuo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gensim.utils import simple_preprocess\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from itertools import chain\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Lambda, Bidirectional, Dropout, TimeDistributed, Reshape, RepeatVector, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "latent_dim = 128\n",
    "embedding_dim = 150\n",
    "epochs = 50\n",
    "min_count_words = 3\n",
    "max_sequence_len=25\n",
    "batch_size=256\n",
    "rnn_first_layer_size = 256\n",
    "rnn_second_layer_size = 128\n",
    "\n",
    "\n",
    "dataset, _ = tfds.load('tiny_shakespeare', with_info=True, as_supervised=False)\n",
    "unsplit_corpus = [simple_preprocess(data['text'].numpy().decode('utf-8')) for data in dataset['train']][0]\n"
   ],
   "metadata": {
    "id": "CWC2Bhg6T2Nj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "MFKeSHxmGkXa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def split_corpus_by_len(corpus, max_sequence_len=max_sequence_len):\n",
    "    # Splits the corpus into segments of max_sequence_len, ensuring no segment exceeds this length.\n",
    "    new_corpus = [corpus[i:i+max_sequence_len] for i in range(0, len(corpus), max_sequence_len)]\n",
    "    new_corpus.pop()  # Removes the last item in case it's shorter than the desired length.\n",
    "    return new_corpus\n",
    "\n",
    "def build_vocab(corpus):\n",
    "    # Builds a vocabulary of words from the corpus, filtering out those with low frequency.\n",
    "    global min_count_words\n",
    "    word_counts = defaultdict(int)\n",
    "    for sequence in corpus:\n",
    "        for word in sequence:\n",
    "            word_counts[word] += 1  # Counts the occurrence of each word in the corpus.\n",
    "    print(word_counts)\n",
    "    # Filters out words whose frequency is below the threshold (min_count_words).\n",
    "    vocab = [word for word, count in word_counts.items() if count >= min_count_words]\n",
    "    return vocab\n",
    "\n",
    "def load_and_preprocess_data(unsplit_corpus):\n",
    "    # Loads and preprocesses the data for use in the model.\n",
    "    corpus = split_corpus_by_len(unsplit_corpus)  # Splits the unsplit corpus into smaller segments.\n",
    "\n",
    "    vocab = build_vocab(corpus)  # Builds the vocabulary from the corpus.\n",
    "    vocab_size = len(vocab)\n",
    "    print(f\"Vocab Size: {vocab_size}\")\n",
    "\n",
    "    # Tokenizes the corpus: converts words to integers based on their frequency.\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(vocab)\n",
    "    sequences = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    # Updates the tokenizer to only include words present in the vocabulary.\n",
    "    tokenizer.word_index = {word: index for word, index in tokenizer.word_index.items() if word in vocab}\n",
    "    tokenizer.index_word = {index: word for word, index in tokenizer.index_word.items() if word in vocab}\n",
    "\n",
    "    # Pads sequences to a uniform length.\n",
    "    max_sequence_len = max([len(x) for x in sequences])\n",
    "    sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='post'))\n",
    "\n",
    "    return corpus, sequences, tokenizer\n",
    "\n",
    "# Main execution: Loads the corpus, preprocesses the data, and retrieves the processed corpus, sequences, and tokenizer.\n",
    "corpus, sequences, tokenizer = load_and_preprocess_data(unsplit_corpus)\n"
   ],
   "metadata": {
    "id": "NItTDXWn9uWr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sequences.shape"
   ],
   "metadata": {
    "id": "KqU2LHWvNmWI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Total Sequences: {len(sequences)}\")"
   ],
   "metadata": {
    "id": "HVn10Vf2bRHm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_sequences, test_sequences = train_test_split(sequences, test_size=0.1)\n",
    "print(f\"Training Sequences: {len(train_sequences)}\")\n",
    "print(f\"Test Sequences: {len(test_sequences)}\")\n"
   ],
   "metadata": {
    "id": "sHZlywFBdxKo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def prepare_dataset(sequences, batch_size=batch_size):\n",
    "    # Prepares a TensorFlow dataset from the given sequences.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sequences, sequences))\n",
    "    # Shuffles the dataset with a buffer of 10000 and batches it according to the specified batch size.\n",
    "    return dataset.shuffle(10000).batch(batch_size)\n",
    "\n",
    "# Prepares the training dataset from the train_sequences, and enables prefetching for performance optimization.\n",
    "train_dataset = prepare_dataset(train_sequences).prefetch(tf.data.AUTOTUNE)\n",
    "# Similar to the training dataset, prepares the test dataset and enables prefetching.\n",
    "test_dataset = prepare_dataset(test_sequences).prefetch(tf.data.AUTOTUNE)\n"
   ],
   "metadata": {
    "id": "HW3KsJy9dzeO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_word_embeddings(corpus):\n",
    "    # Creates word embeddings from the given corpus using the Word2Vec model.\n",
    "    global min_count_words\n",
    "    # Initializes the Word2Vec model with specified parameters:\n",
    "    # vector_size: the dimensionality of the word vectors,\n",
    "    # window: the maximum distance between the current and predicted word within a sentence,\n",
    "    # min_count: ignores all words with total frequency lower than this,\n",
    "    # workers: use this many worker threads to train the model (=faster training with multicore machines),\n",
    "    # epochs: number of iterations (epochs) over the corpus.\n",
    "    word2vec_model = Word2Vec(corpus, vector_size=embedding_dim, window=5, min_count=min_count_words, workers=4, epochs=100)\n",
    "    return word2vec_model\n",
    "\n",
    "# Generates the word embeddings using the corpus with the create_word_embeddings function.\n",
    "word2vec_model = create_word_embeddings(corpus)\n"
   ],
   "metadata": {
    "id": "tcHgBXwMbAYu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "word2vec_model.wv.vectors.shape"
   ],
   "metadata": {
    "id": "ar09zqZ-1myZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_embedding_matrix(word2vec_model, tokenizer, embedding_dim):\n",
    "    # Creates an embedding matrix that maps each word to its vector representation.\n",
    "    vocab_size = len(tokenizer.word_index) + 1  # The size of the vocabulary including a reserved index at 0.\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))  # Initializes the matrix with zeros.\n",
    "\n",
    "    # Iterates over each word in the tokenizer's vocabulary.\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if word in word2vec_model.wv:\n",
    "            # Retrieves the word's embedding from the Word2Vec model.\n",
    "            embedding_vector = word2vec_model.wv[word]\n",
    "            # Places the word's embedding vector in the corresponding row of the matrix.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix  # Returns the complete embedding matrix.\n",
    "\n",
    "# Creates an embedding matrix for the given Word2Vec model and tokenizer.\n",
    "embedding_matrix = create_embedding_matrix(word2vec_model, tokenizer, embedding_dim)\n"
   ],
   "metadata": {
    "id": "2wbGgXBybFh1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "embedding_matrix.shape"
   ],
   "metadata": {
    "id": "gkvh4lYCcMf4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocab_size = embedding_matrix.shape[0]  # Retrieves the size of the vocabulary from the embedding matrix.\n",
    "\n",
    "def build_vae(embedding_matrix, max_sequence_len=max_sequence_len, latent_dim=latent_dim):\n",
    "    vocab_size = embedding_matrix.shape[0]  # Vocabulary size (number of unique tokens).\n",
    "    embedding_dim = embedding_matrix.shape[1]  # Dimension of each word vector.\n",
    "\n",
    "    # Defines the embedding layer using the pre-trained word embeddings.\n",
    "    embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                                embedding_matrix.shape[1],\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_len,\n",
    "                                trainable=False)\n",
    "\n",
    "    # Encoder part of the VAE.\n",
    "    encoder_inputs = Input(shape=(max_sequence_len,))\n",
    "    x = embedding_layer(encoder_inputs)\n",
    "    x = Bidirectional(LSTM(256, return_sequences=True))(x)  # Bidirectional LSTM layer.\n",
    "    x = Dropout(0.15)(x)  # Dropout layer for regularization.\n",
    "    x = Bidirectional(LSTM(128))(x)  # Another LSTM layer.\n",
    "    z_mean = Dense(latent_dim)(x)  # Dense layer to generate z_mean.\n",
    "    z_log_var = Dense(latent_dim)(x)  # Dense layer to generate z_log_var.\n",
    "\n",
    "    # Sampling function to generate the latent vector.\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # Decoder part of the VAE.\n",
    "    decoder_inputs = Input(shape=(latent_dim,))\n",
    "    x = Dense(128, activation='relu')(decoder_inputs)\n",
    "    x = RepeatVector(max_sequence_len)(x)  # Repeats the vector for sequence generation.\n",
    "    x = LSTM(64, return_sequences=True)(x)  # LSTM layer in the decoder.\n",
    "    x = Dense(max_sequence_len * vocab_size, activation='relu')(x)\n",
    "    decoder_outputs = TimeDistributed(Dense(vocab_size, activation='softmax'))(x)  # TimeDistributed Dense layer.\n",
    "\n",
    "    # Assembling the VAE model.\n",
    "    encoder = Model(encoder_inputs, [z_mean, z_log_var, z])\n",
    "    decoder = Model(decoder_inputs, decoder_outputs)\n",
    "    vae = Model(encoder_inputs, decoder(encoder(encoder_inputs)[2]))\n",
    "\n",
    "    return vae, encoder, decoder\n",
    "\n",
    "# Builds the VAE using the embedding matrix.\n",
    "vae, encoder, decoder = build_vae(embedding_matrix=embedding_matrix)\n",
    "\n",
    "class VAELoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, encoder, vocab_size, **kwargs):\n",
    "        super(VAELoss, self).__init__(**kwargs)\n",
    "        self.encoder = encoder  # The encoder part of the VAE.\n",
    "        self.vocab_size = vocab_size  # Vocabulary size.\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        z_mean, z_log_var, _ = self.encoder(y_true)\n",
    "\n",
    "        # Converts y_true to one-hot encoding.\n",
    "        y_true_one_hot = tf.one_hot(tf.cast(tf.squeeze(y_true), tf.int32), depth=self.vocab_size)\n",
    "\n",
    "        # Binary cross-entropy for reconstruction loss.\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            tf.keras.losses.categorical_crossentropy(y_true_one_hot, y_pred))\n",
    "\n",
    "        # KL divergence for regularization.\n",
    "        kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
    "\n",
    "        return reconstruction_loss + kl_loss  # Total VAE loss.\n",
    "\n",
    "# Custom loss function for the VAE.\n",
    "loss_function = VAELoss(encoder, vocab_size=vocab_size)\n",
    "\n",
    "# Compiling and summarizing the VAE model.\n",
    "vae.compile(optimizer=Adam(0.01), loss=loss_function)\n",
    "vae.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}