{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNR2jczjxZnjPwc+DAMNPjM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3uNofyYd4GtD"},"outputs":[],"source":["import numpy as np\n","from tensorflow.keras.datasets import reuters\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, LSTM, RepeatVector, TimeDistributed, Embedding, Dropout, Bidirectional\n","from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.optimizers import Adam\n","\n","# Parameters\n","vocab_size = 10000\n","max_seq_length = 100  # Maximum length of sequences\n","embedding_dim = 150   # Size of word embeddings\n","latent_dim = 128     # Size of the latent space\n","rnn_cells_first = 256  # Number of cells in the first RNN layer\n","rnn_cells_second = 128  # Number of cells in the second RNN layer\n","epochs = 100\n","batch_size = 256\n"]},{"cell_type":"code","source":["# Load Reuters dataset\n","(x_train, _), (x_test, _) = reuters.load_data(num_words=vocab_size)\n","x_train = pad_sequences(x_train, maxlen=max_seq_length, padding='post')\n","x_test = pad_sequences(x_test, maxlen=max_seq_length, padding='post')"],"metadata":{"id":"K4INsEiLKcWF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Encoder\n","input_text = Input(shape=(max_seq_length,))\n","x = None  # Embedding layer\n","x = None  # Apply Bidirectional LSTM layer, remember to return all steps of the sequence\n","x = None  # Apply dropout regularization\n","x = None  # Apply another Bidirectional LSTM\n","latent_repr = Dense(latent_dim, activation='relu')(x)  # Dense layer with ReLU activation\n","encoder_model = Model(input_text, latent_repr)  # Define encoder model\n","encoder_model.summary()  # Display model summary"],"metadata":{"id":"sywm3KoBKd0W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Decoder\n","decoder_input = Input(shape=(latent_dim,))  # Define decoder input layer with shape (latent_dim,)\n","decoder_x = None  # Dense layer to transform decoder input, what shape should it have?\n","decoder_x = None  # Repeat the vector to match sequence length\n","decoder_x = None  # Apply LSTM to decode sequences\n","decoder_x = None  # Apply dropout regularization to prevent overfitting\n","decoder_x = None  # Apply another LSTM layer for sequence generation, return sequences again to avoid repeat layer\n","decoder_output = None  # Dense layer for output sequence prediction with softmax activation\n","decoder_model = Model(decoder_input, decoder_output)  # Define decoder model\n","decoder_model.summary()  # Display model summary\n"],"metadata":{"id":"gQDW3kRo6MvF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Autoencoder\n","autoencoder = Model(input_text, decoder_model(encoder_model(input_text)))\n","autoencoder.compile(optimizer=Adam(learning_rate=0.0005), loss='sparse_categorical_crossentropy')\n","autoencoder.summary()"],"metadata":{"id":"RfGA489U6NyB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","from tensorflow.keras.callbacks import LearningRateScheduler\n","import tensorflow.keras.backend as K\n","\n","def custom_lr_scheduler(epoch, lr):\n","    # Decrease learning rate by 0.1 factor every 5 epochs\n","    pass\n","\n","# Define the callback\n","lr_scheduler = None\n","\n","early_stopping = None  # Define EarlyStopping on val_loss\n","\n","# Train the model\n","autoencoder.fit(None)\n"],"metadata":{"id":"lYT_XZcn6Fen"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Encode the input text using the encoder model\n","encoded_texts = encoder_model.predict(x_test[:10])\n","\n","# Decode the encoded texts using the decoder model\n","# This gives you a probability distribution for each word in the sequence\n","prob_distributions = decoder_model.predict(encoded_texts)\n"],"metadata":{"id":"rKv2NwBG4Nc-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sample(preds, temperature=1.0):\n","    # Convert to array and prevent numerical issues with very small numbers\n","    preds = np.asarray(preds).astype('float64')\n","    preds = np.log(preds + 1e-7) / temperature  # Adjust by temperature\n","    exp_preds = np.exp(preds)\n","    preds = exp_preds / np.sum(exp_preds)  # Softmax\n","    probas = np.random.multinomial(1, preds, 1)  # Sample from the softmax distribution\n","    return np.argmax(probas)\n"],"metadata":{"id":"97e44eGbBIz6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get word_index and reverse_word_index\n","\n","word_index = None\n","reverse_word_index = None\n","\n","def decode_sequence_with_sampling(prob_distributions, temperature=1.0):\n","    # Implement decoding with sampling\n","    return pass\n"],"metadata":{"id":"L0lAbwbvBLYf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Example usage with a specified temperature\n","temperature = 1  # Adjust this value to change randomness\n","decoded_texts = [decode_sequence_with_sampling(seq, temperature) for seq in prob_distributions]\n","for text in decoded_texts:\n","    print('\\n')\n","    print(f'Decoded: {text}')\n","    print('\\n')\n"],"metadata":{"id":"n5mCyPm55_mD"},"execution_count":null,"outputs":[]}]}